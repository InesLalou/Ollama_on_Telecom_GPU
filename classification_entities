import os
import pandas as pd
import pprint 
from ollama import Client
from time import sleep 
from sklearn.metrics import classification_report

BASE_PATH = "C:/Users/ine28/OneDrive/Documents/Mast√®re_IA/Projet_Fil_Rouge/dataset_val"  # Exemple : "/home/infres/lalou-24/experiments/dataset/"

languages = ["EN", "RU", "PT", "HI", "BG"]

dataset_test_EN = []
dataset_test_RU = []
dataset_test_PT = []
dataset_test_HI = []
dataset_test_BG = []

dataset_global = {}

few_shot_examples = """
Tu es un assistant qui aide √† annoter les entit√©s nomm√©s d'un texte selon 3 r√¥les : antagonist, protagonist, innocent.
Voici des exemples annot√©s avec le r√¥le des entit√©s dans leur contexte :

Texte : "KHARKIV - Russia pounded over 30 villages and towns in Ukraine‚Äôs northeastern Kharkiv region after launching a ground offensive in the border region, forcing almost 6,000 to evacuate, the governor said on Monday."
Entit√© : "Russia"
R√¥le : Antagonist

Texte : "Alsynov has previously criticized military mobilization in the region as ‚Äúgenocide‚Äù against the Bashkir people."
Entit√© : "Bashkir people"
R√¥le : Innocent

Texte : "In this event, we show the world we exist: we are Ukraine, a real powerful, independent and democratic country,\" said Valeriy Sushkevych."
Entit√© : "Ukraine"
R√¥le : Protagonist

"""


nb_prediction = 91 


# Traiter chaque langue
for lang in languages:
    dataset = []
    print(f"Traitement de la langue : {lang}")
    lang_path = os.path.join(BASE_PATH, lang)
    
    # fichier d'annotations
    mentions_path = os.path.join(lang_path, "subtask-1-annotations.txt")
    if not os.path.exists(mentions_path):
        print(f"Fichier d‚Äôannotations manquant pour {lang}")
        continue

    # dossier contenant les documents texte
    documents_path = os.path.join(lang_path, "subtask-1-documents")
    if not os.path.exists(documents_path):
        print(f"Dossier de documents manquant pour {lang}")
        continue

    annotations = pd.read_csv(
    mentions_path,
    sep="\t",
    header=None,
    engine='python',          # Pour g√©rer les lignes avec colonnes variables
    usecols=[0,1,2,3,4],     # Prend uniquement les 5 premi√®res colonnes
    names=["file", "entity", "start", "end", "true_role"]
)


    # tous les textes
    text_cache = {}
    for file_name in annotations["file"].unique():
        text_file_path = os.path.join(documents_path, file_name)
        if os.path.exists(text_file_path):
            with open(text_file_path, "r", encoding="utf-8") as f:
                text_cache[file_name] = f.read()
        else:
            print(f"Texte introuvable : {text_file_path}")

    grouped = annotations.groupby("file")

    for file_name, group in grouped:
        if file_name not in text_cache:
            continue
        text = text_cache[file_name]
        entities = []
        for _, row in group.iterrows():
            entities.append({
                "entity": row["entity"],
                "start": int(row["start"]),
                "end": int(row["end"]),
                "true_role": row["true_role"]
            })

        dataset.append({
            "file": file_name,
            "text": text,
            "entities": entities
        })
    dataset_global[lang] = dataset


# test affichage et nb d'entit√©s
pprint.pprint(dataset_global.get("EN", [])[:1])
pprint.pprint(dataset_global.get("RU", [])[:1])
pprint.pprint(dataset_global.get("PT", [])[:1])
pprint.pprint(dataset_global.get("HI", [])[:1])
pprint.pprint(dataset_global.get("BG", [])[:1])

if "EN" in dataset_global:
    nb_entities_EN = sum(len(doc["entities"]) for doc in dataset_global["EN"])
    print(f"Nombre total d'entit√©s en EN : {nb_entities_EN}")
if "RU" in dataset_global:
    nb_entities_EN = sum(len(doc["entities"]) for doc in dataset_global["RU"])
    print(f"Nombre total d'entit√©s en RU : {nb_entities_EN}")
if "PT" in dataset_global:
    nb_entities_EN = sum(len(doc["entities"]) for doc in dataset_global["PT"])
    print(f"Nombre total d'entit√©s en PT : {nb_entities_EN}")
if "HI" in dataset_global:
    nb_entities_EN = sum(len(doc["entities"]) for doc in dataset_global["HI"])
    print(f"Nombre total d'entit√©s en HI : {nb_entities_EN}")
if "BG" in dataset_global:
    nb_entities_EN = sum(len(doc["entities"]) for doc in dataset_global["BG"])
    print(f"Nombre total d'entit√©s en BG : {nb_entities_EN}")


client = Client(host='http://localhost:11434')

print(f"\n--- Pr√©dictions pour les {nb_prediction} premi√®res entit√©s EN ---\n")

results = []

count = 0
for doc in dataset_global.get("EN", []):
    if count >= nb_prediction:
        break
    text = doc["text"]
    for entity in doc["entities"]:
        if count >= nb_prediction:
            break

        entity_text = entity["entity"]
        entity_start = entity["start"]
        entity_end = entity["end"]
        true_role = entity["true_role"]

        entity_snippet = text[max(0, entity_start - 150):min(len(text), entity_end + 150)]

        prompt = f"""{few_shot_examples}

        Voici un nouveau cas √† analyser :
        Texte : \"{entity_snippet}\"
        Entit√© : \"{entity_text}\" (position : {entity_start}-{entity_end} dans le texte complet)
        Quel est le r√¥le de cette entit√© dans ce contexte ?
        
        R√©pond uniquement par l'un des mots suivants : Protagonist, Antagonist, Innocent. 
        R√¥le :"""

        try:
            response = client.chat(
                model="mistral",
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            predicted_role_raw = response['message']['content'].strip()

            # Nettoyage de la pr√©diction
            valid_roles = {"Protagonist", "Antagonist", "Innocent"}
            predicted_clean = predicted_role_raw.lower().split()[0]  # Prend juste le premier mot

            if predicted_clean not in valid_roles:
                print(f"‚ö†Ô∏è R√©ponse inattendue : '{predicted_role_raw}' ‚Üí ignor√©e ou marqu√©e comme 'unknown'")
                predicted_clean = "unknown"  # Tu peux aussi faire `continue` si tu veux les exclure

            print(f"[{count+1}] Entit√©: {entity_text} ‚Üí R√¥le pr√©dit: {predicted_clean} | R√¥le r√©el: {true_role}")

            results.append({
                "file": doc["file"],
                "entity": entity_text,
                "start": entity_start,
                "end": entity_end,
                "true_role": true_role.lower(),
                "predicted_role": predicted_clean
            })

        except Exception as e:
            print(f"Erreur √† l'entit√© {count+1}: {e}")

        count += 1
        sleep(0.5)


# Evaluation
results_df = pd.DataFrame(results)
results_df.to_csv("predictions_vs_truth.csv", index=False, encoding="utf-8")
print("\nüìÑ R√©sultats enregistr√©s dans predictions_vs_truth.csv")

true_labels = [r["true_role"].lower() for r in results]
predicted_labels = [r["predicted_role"].lower() for r in results]


report = classification_report(true_labels, predicted_labels, digits=4)
print("\n--- Rapport de classification ---")
print(report)

